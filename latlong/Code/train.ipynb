{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eba2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image \n",
    "import timm \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision import models \n",
    "\n",
    "# --- Helper Function to Initialize EfficientNet-B0 (Used for Frozen Model) ---\n",
    "# (Using the robust version provided)\n",
    "def get_model(model_name, num_classes, pretrained=False, checkpoint_path=None, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Initializes a torchvision EfficientNet, modifies classifier, and loads state_dict.\n",
    "    Handles 'module.' prefix and potential dictionary wrapping in checkpoint.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the EfficientNet model (e.g., 'efficientnet_b0').\n",
    "        num_classes (int): Number of output classes for the final classifier.\n",
    "        pretrained (bool): If True AND checkpoint_path is None, load torchvision pretrained weights.\n",
    "                           If checkpoint_path is provided, this is ignored for loading state_dict.\n",
    "        checkpoint_path (str, optional): Path to a .pth file to load state_dict.\n",
    "        device (str or torch.device): Device to load the model and weights onto.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The initialized (and potentially loaded) model.\n",
    "    \"\"\"\n",
    "    print(f\"--- Initializing base model: {model_name} for {num_classes} classes ---\")\n",
    "    if model_name == \"efficientnet_b0\":\n",
    "        # Use weights=None if loading a custom checkpoint,\n",
    "        # or specify weights if using torchvision pretraining without a checkpoint\n",
    "        weights = 'IMAGENET1K_V1' if pretrained and checkpoint_path is None else None\n",
    "        model = models.efficientnet_b0(weights=weights)\n",
    "        try:\n",
    "            num_ftrs = model.classifier[1].in_features\n",
    "            model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
    "            print(f\"   Replaced classifier head for {num_classes} classes.\")\n",
    "        except Exception as e:\n",
    "             raise ValueError(f\"Could not modify torchvision efficientnet_b0 classifier[1]: {e}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "    model = model.to(device) # Move model skeleton to device first\n",
    "\n",
    "    if checkpoint_path:\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            raise FileNotFoundError(f\"Checkpoint file not found: {checkpoint_path}\")\n",
    "\n",
    "        print(f\"--- Loading specific checkpoint: {checkpoint_path}\")\n",
    "        # Load state_dict to the specified device directly\n",
    "        state_dict_loaded = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "        # Handle dictionary wrapping (common practice)\n",
    "        if isinstance(state_dict_loaded, dict) and 'state_dict' in state_dict_loaded:\n",
    "            state_dict = state_dict_loaded['state_dict']\n",
    "            print(\"   (Loaded from 'state_dict' key)\")\n",
    "        elif isinstance(state_dict_loaded, dict) and 'model_state_dict' in state_dict_loaded:\n",
    "             state_dict = state_dict_loaded['model_state_dict']\n",
    "             print(\"   (Loaded from 'model_state_dict' key)\")\n",
    "        elif isinstance(state_dict_loaded, dict) and not any(k in state_dict_loaded for k in ['state_dict', 'model_state_dict']):\n",
    "             # If it's a dict but doesn't have the common keys, assume it *is* the state_dict\n",
    "             state_dict = state_dict_loaded\n",
    "             print(\"   (Loaded dictionary directly as state_dict)\")\n",
    "        elif isinstance(state_dict_loaded, nn.Module): # Less common, but possible\n",
    "            state_dict = state_dict_loaded.state_dict()\n",
    "            print(\"   (Loaded state_dict from a saved nn.Module object)\")\n",
    "        else:\n",
    "             # Assume it's the state_dict directly if not a dict or known wrapper\n",
    "             state_dict = state_dict_loaded\n",
    "             print(\"   (Loaded unknown format directly, assuming state_dict)\")\n",
    "\n",
    "\n",
    "        # Handle 'module.' prefix (from DataParallel saving)\n",
    "        new_state_dict = {}\n",
    "        prefix_found = False\n",
    "        for k, v in state_dict.items():\n",
    "            if k.startswith('module.'):\n",
    "                name = k[7:] # remove `module.`\n",
    "                prefix_found = True\n",
    "            else:\n",
    "                name = k\n",
    "            new_state_dict[name] = v\n",
    "        if prefix_found: print(\"   (Handled 'module.' prefix in keys)\")\n",
    "\n",
    "        try:\n",
    "            # Use strict=True when loading specific model weights\n",
    "            missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=True)\n",
    "            # It's expected that a checkpoint might only contain weights, not buffers etc.\n",
    "            # However, for a full model checkpoint, both lists should ideally be empty.\n",
    "            if missing_keys: print(f\"   WARNING: Missing keys during load_state_dict: {missing_keys}\")\n",
    "            if unexpected_keys: print(f\"   WARNING: Unexpected keys during load_state_dict: {unexpected_keys}\")\n",
    "            print(f\"✅ Loaded checkpoint weights into model structure.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ ERROR loading state_dict into model structure: {e}\")\n",
    "            # Print keys for debugging if loading fails\n",
    "            print(\"   Model keys (first 5):\", list(model.state_dict().keys())[:5], \"...\")\n",
    "            print(\"   Checkpoint keys (first 5):\", list(new_state_dict.keys())[:5], \"...\")\n",
    "            raise RuntimeError(f\"Failed to load checkpoint {checkpoint_path} into {model_name}\") from e\n",
    "\n",
    "    elif pretrained:\n",
    "         # This case is hit if pretrained=True BUT no checkpoint_path is given.\n",
    "         # The model was already initialized with weights='IMAGENET1K_V1' above.\n",
    "         print(f\"✅ Using torchvision ImageNet pretrained weights for {model_name}.\")\n",
    "    else:\n",
    "        print(f\"   Model {model_name} initialized with random weights (pretrained=False, no checkpoint).\")\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- Combined Geo-Prediction Model ---\n",
    "class CombinedGeoModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 frozen_region_model_path,\n",
    "                 num_region_classes=15,\n",
    "                 head_hidden_dims=[512, 128],\n",
    "                 use_softmax_for_region=True, \n",
    "                 trainable_effnet_name='efficientnet_b0',\n",
    "                 device='cuda'):\n",
    "        \"\"\"\n",
    "        Initializes the combined model.\n",
    "\n",
    "        Args:\n",
    "            frozen_region_model_path (str): Path to the .pth file for the frozen Region_ID model.\n",
    "            num_region_classes (int): Number of classes for the Region_ID model.\n",
    "            head_hidden_dims (list): List of hidden layer dimensions for the regression head.\n",
    "            use_softmax_for_region (bool): Whether to use softmax output (True) or logits (False)\n",
    "                                           from the frozen region model.\n",
    "            trainable_effnet_name (str): Name of the EfficientNet model to use for trainable features.\n",
    "            device (str or torch.device): Device to run the model on.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_region_classes = num_region_classes\n",
    "        self.use_softmax_for_region = use_softmax_for_region\n",
    "        self.device = device\n",
    "\n",
    "        # 1. Load and Freeze the Region_ID Classifier Model\n",
    "        print(\"--- Loading Frozen Region Model ---\")\n",
    "        self.frozen_region_model = get_model( # Uses the corrected get_model function\n",
    "            model_name='efficientnet_b0', # Explicitly use EffNet-B0 for region model\n",
    "            num_classes=num_region_classes,\n",
    "            pretrained=False, # Must be False when loading specific checkpoint\n",
    "            checkpoint_path=frozen_region_model_path, # Path to the specific frozen model weights\n",
    "            device=self.device # Load directly to the target device\n",
    "            )\n",
    "        # Freeze all parameters in the region model\n",
    "        for param in self.frozen_region_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.frozen_region_model.eval() # Set to evaluation mode permanently\n",
    "        print(\"Frozen Region Model loaded and parameters frozen.\")\n",
    "\n",
    "        # 2. Load the Trainable Image Embedding Model (using timm)\n",
    "        print(\"\\n--- Loading Trainable Image Embedder ---\")\n",
    "        self.trainable_image_embedder = timm.create_model(\n",
    "            trainable_effnet_name,\n",
    "            pretrained=True # Use ImageNet pretraining for the embedder\n",
    "        )\n",
    "        # Get the embedding dimension (before the classifier)\n",
    "        self.embedding_dim = self.trainable_image_embedder.get_classifier().in_features\n",
    "        # Replace the classifier with an Identity layer to get the features\n",
    "        self.trainable_image_embedder.reset_classifier(0, '') # Efficient way in timm to remove classifier\n",
    "        # self.trainable_image_embedder.classifier = nn.Identity() # Alternative way\n",
    "        print(f\"Trainable Image Embedder ({trainable_effnet_name}) loaded with output dim: {self.embedding_dim}\")\n",
    "        self.trainable_image_embedder = self.trainable_image_embedder.to(self.device) # Move embedder to device\n",
    "\n",
    "\n",
    "\n",
    "        # ***** START FIX *****\n",
    "        # Define pooling and flattening layers HERE in __init__\n",
    "        self.pool = nn.AdaptiveAvgPool2d(output_size=1).to(self.device)\n",
    "        self.flat = nn.Flatten(start_dim=1).to(self.device)\n",
    "        # ***** END FIX *****\n",
    "\n",
    "        # 3. Define the Regression Head\n",
    "        print(\"\\n--- Defining Regression Head ---\")\n",
    "        # Input dimension is embedding dim + region features (either logits or probabilities)\n",
    "        input_head_dim = self.embedding_dim + self.num_region_classes\n",
    "        print(f\"Regression head input dimension: {input_head_dim} ({self.embedding_dim} from embedder + {self.num_region_classes} from region model)\")\n",
    "        layers = []\n",
    "        current_dim = input_head_dim\n",
    "        for hidden_dim in head_hidden_dims:\n",
    "            layers.append(nn.Linear(current_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim)) # Batch norm often helps in regression heads\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.3)) # Dropout for regularization\n",
    "            current_dim = hidden_dim\n",
    "\n",
    "        # Final output layer for 2 values (lat, lon)\n",
    "        layers.append(nn.Linear(current_dim, 2))\n",
    "        self.regression_head = nn.Sequential(*layers).to(self.device) # Move head to device\n",
    "        print(f\"Regression head structure: {self.regression_head}\")\n",
    "        print(\"\\nModel Initialization Complete.\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the combined model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input image tensor (B, C, H, W) already on the correct device.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Predicted scaled latitude and longitude (B, 2).\n",
    "        \"\"\"\n",
    "        # 1. Get Region_ID features (logits or softmax) from the frozen model\n",
    "        # Ensure no gradients are computed for this part\n",
    "        with torch.no_grad():\n",
    "            region_logits = self.frozen_region_model(x)\n",
    "            if self.use_softmax_for_region:\n",
    "                region_features = F.softmax(region_logits, dim=1)\n",
    "            else:\n",
    "                region_features = region_logits # Use logits directly\n",
    "\n",
    "        # 2. Get image embeddings from the trainable model\n",
    "        # Gradients will flow back through this part\n",
    "        image_features = self.trainable_image_embedder(x) # timm model output is features directly now\n",
    "\n",
    "        # ***** START FIX *****\n",
    "        # Check dimensions and apply pooling/flattening if necessary\n",
    "        if image_features.ndim == 4:\n",
    "            # print(\"Applying pool and flatten to 4D embedder output.\") # Optional debug print\n",
    "            image_features = self.pool(image_features) # Shape: (B, C, 1, 1)\n",
    "            image_features = self.flat(image_features) # Shape: (B, C), 2D\n",
    "        elif image_features.ndim != 2:\n",
    "             # If it's not 4D or 2D, something unexpected happened\n",
    "             raise RuntimeError(f\"Unexpected dimension for image_features from embedder: {image_features.ndim}. Expected 2D or 4D. Shape: {image_features.shape}\")\n",
    "        # Now image_features should reliably be 2D: (B, embedding_dim)\n",
    "        # ***** END FIX *****\n",
    "\n",
    "\n",
    "        # 3. Concatenate features\n",
    "        # Ensure dimensions match: (B, embedding_dim), (B, num_region_classes)\n",
    "        combined_features = torch.cat((image_features, region_features), dim=1)\n",
    "\n",
    "        # 4. Pass through the regression head\n",
    "        output = self.regression_head(combined_features)\n",
    "\n",
    "        return output\n",
    "\n",
    "# --- Custom Dataset with Exclusion Logic ---\n",
    "class GeoImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None, scaler_lat=None, scaler_lon=None, exclude_filenames=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            img_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            scaler_lat (StandardScaler): Fitted scaler for latitude. Can be None if data is pre-scaled.\n",
    "            scaler_lon (StandardScaler): Fitted scaler for longitude. Can be None if data is pre-scaled.\n",
    "            exclude_filenames (list, optional): A list of filenames (e.g., 'img_0095.jpg') to exclude.\n",
    "        \"\"\"\n",
    "        print(f\"--- Loading dataset from: {csv_file}\")\n",
    "        try:\n",
    "            self.data_frame = pd.read_csv(csv_file)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: CSV file not found at {csv_file}\")\n",
    "            raise\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.scaler_lat = scaler_lat\n",
    "        self.scaler_lon = scaler_lon\n",
    "\n",
    "        # Ensure lat/lon columns exist\n",
    "        if 'latitude' not in self.data_frame.columns or 'longitude' not in self.data_frame.columns:\n",
    "             raise ValueError(f\"CSV '{csv_file}' must contain 'latitude' and 'longitude' columns.\")\n",
    "        if 'filename' not in self.data_frame.columns:\n",
    "             raise ValueError(f\"CSV '{csv_file}' must contain a 'filename' column.\")\n",
    "\n",
    "        # --- Exclusion Logic ---\n",
    "        if exclude_filenames:\n",
    "            initial_len = len(self.data_frame)\n",
    "            self.data_frame = self.data_frame[~self.data_frame['filename'].isin(exclude_filenames)].reset_index(drop=True)\n",
    "            print(f\"   Excluded {initial_len - len(self.data_frame)} rows based on exclude_filenames list.\")\n",
    "            if len(self.data_frame) == 0:\n",
    "                 print(f\"WARNING: All rows were excluded from {csv_file}. Check your exclude_filenames list.\")\n",
    "\n",
    "        # Pre-scale labels if scalers are provided\n",
    "        if self.scaler_lat and self.scaler_lon:\n",
    "             print(\"   Scaling latitude and longitude using provided scalers.\")\n",
    "             try:\n",
    "                self.scaled_lat = self.scaler_lat.transform(self.data_frame[['latitude']]).flatten()\n",
    "                self.scaled_lon = self.scaler_lon.transform(self.data_frame[['longitude']]).flatten()\n",
    "             except Exception as e:\n",
    "                 print(f\"Error during scaling: {e}\")\n",
    "                 print(\"Ensure scalers were fitted correctly on the training data.\")\n",
    "                 raise\n",
    "        else:\n",
    "             # If scalers not provided, assume columns 'latitude', 'longitude' are ALREADY scaled\n",
    "             print(\"   WARNING: Scalers not provided. Assuming 'latitude' and 'longitude' columns in CSV are already scaled.\")\n",
    "             self.scaled_lat = self.data_frame['latitude'].values\n",
    "             self.scaled_lon = self.data_frame['longitude'].values\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_filename = self.data_frame.loc[idx, 'filename']\n",
    "        img_name = os.path.join(self.img_dir, img_filename)\n",
    "        try:\n",
    "            # Use PIL Image open for broader compatibility, then convert\n",
    "            image = Image.open(img_name).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "             print(f\"Error: Image not found at {img_name} (referenced in CSV row {idx})\")\n",
    "             # You might want to return None and handle in collate_fn, or raise error\n",
    "             raise FileNotFoundError(f\"Image not found: {img_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening or processing image {img_name}: {e}\")\n",
    "            raise # Re-raise other image loading errors\n",
    "\n",
    "        # Apply transforms if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Get scaled labels\n",
    "        # Ensure index is valid after potential filtering\n",
    "        if idx >= len(self.scaled_lat):\n",
    "             raise IndexError(f\"Index {idx} out of bounds after filtering/scaling. Dataset length: {len(self)}. Scaled lat length: {len(self.scaled_lat)}\")\n",
    "\n",
    "        scaled_lat = self.scaled_lat[idx]\n",
    "        scaled_lon = self.scaled_lon[idx]\n",
    "        labels = torch.tensor([scaled_lat, scaled_lon], dtype=torch.float32)\n",
    "\n",
    "        # Return image filename along with data for potential debugging\n",
    "        # return image, labels, img_filename\n",
    "        return image, labels\n",
    "\n",
    "\n",
    "# --- Weighted Loss Function ---\n",
    "class WeightedLatLonLoss(nn.Module):\n",
    "    def __init__(self, lat_weight=0.4, lon_weight=0.6):\n",
    "        super().__init__()\n",
    "        if abs((lat_weight + lon_weight) - 1.0) > 1e-6:\n",
    "             print(f\"WARNING: Weights ({lat_weight}, {lon_weight}) do not sum close to 1. Normalizing.\")\n",
    "             total = lat_weight + lon_weight\n",
    "             lat_weight = lat_weight / total\n",
    "             lon_weight = lon_weight / total\n",
    "\n",
    "        self.lat_weight = lat_weight\n",
    "        self.lon_weight = lon_weight\n",
    "        self.mse = nn.MSELoss() # Use MSE as the base loss for each component\n",
    "        # self.huber = nn.HuberLoss() # Alternatively, use Huber loss\n",
    "        print(f\"Initialized WeightedLatLonLoss with lat_weight={self.lat_weight:.2f}, lon_weight={self.lon_weight:.2f}\")\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        # Ensure predictions and targets have shape (Batch, 2)\n",
    "        if preds.shape != targets.shape or preds.ndim != 2 or preds.shape[1] != 2:\n",
    "             raise ValueError(f\"Shape mismatch or incorrect dimensions for WeightedLatLonLoss. \"\n",
    "                              f\"Preds shape: {preds.shape}, Targets shape: {targets.shape}. Expected (B, 2).\")\n",
    "\n",
    "        lat_loss = self.mse(preds[:, 0], targets[:, 0])\n",
    "        lon_loss = self.mse(preds[:, 1], targets[:, 1])\n",
    "\n",
    "        # Use Huber loss if defined:\n",
    "        # lat_loss = self.huber(preds[:, 0], targets[:, 0])\n",
    "        # lon_loss = self.huber(preds[:, 1], targets[:, 1])\n",
    "\n",
    "        total_loss = self.lat_weight * lat_loss + self.lon_weight * lon_loss\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "# --- Setup & Example Usage ---\n",
    "\n",
    "# 1. Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"=========================================\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"=========================================\\n\")\n",
    "\n",
    "# --- USER-DEFINED PATHS --- >>> PLEASE CHANGE THESE <<<\n",
    "TRAIN_IMG_DIR = \"/kaggle/input/latlong-dataset/images_train_combine\"\n",
    "TRAIN_CSV_PATH = \"/kaggle/input/latlong-dataset/train_combine.csv\"\n",
    "VAL_IMG_DIR = \"/kaggle/input/val-dataset/images_val\"\n",
    "VAL_CSV_PATH = \"/kaggle/input/val-dataset/labels_val.csv\"\n",
    "FROZEN_MODEL_PATH = \"/kaggle/input/saved-models/kaggle/working/saved_models/best_efficientnet_b0.pt\" # Checkpoint for Region Classification model\n",
    "SCALER_SAVE_DIR = \"/kaggle/working/\" # Directory to save fitted scalers\n",
    "MODEL_SAVE_DIR = \"/kaggle/working/combined_model_checkpoints\"\n",
    "# --- END USER-DEFINED PATHS ---\n",
    "\n",
    "NUM_REGION_CLASSES = 15 # Should match the output classes of the frozen model\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4 # Starting learning rate\n",
    "NUM_EPOCHS = 40 # Set to a lower number for quick testing, increase for real training\n",
    "IMG_SIZE = 256 # Input image size for the models\n",
    "HEAD_HIDDEN_DIMS = [512, 256, 128] # Hidden dimensions for the regression head\n",
    "LAT_LOSS_WEIGHT = 0.4 # Weight for latitude component in the loss\n",
    "LON_LOSS_WEIGHT = 0.6 # Weight for longitude component in the loss\n",
    "TRAINABLE_EMBEDDER_NAME = 'efficientnet_b0' # Base model for trainable embedder\n",
    "USE_SOFTMAX_REGION = True # Use softmax output from frozen model (True) or logits (False)\n",
    "\n",
    "# Create model save directory if it doesn't exist\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(SCALER_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# --- Filenames to exclude from validation set ---\n",
    "# Based on IDs: [95, 145, 146, 158, 159, 160, 161]\n",
    "# Assuming filenames follow the pattern 'img_XXXX.jpg' (4 digits with padding)\n",
    "ids_to_exclude = [95, 145, 146, 158, 159, 160, 161]\n",
    "validation_exclude_filenames = [f\"img_{id:04d}.jpg\" for id in ids_to_exclude]\n",
    "print(f\"\\n--- Files to exclude from Validation Set: {validation_exclude_filenames} ---\")\n",
    "\n",
    "\n",
    "# 2. Data Preprocessing & Scaling\n",
    "# IMPORTANT: Fit scalers ONLY on the TRAINING data!\n",
    "scaler_lat = StandardScaler()\n",
    "scaler_lon = StandardScaler()\n",
    "scaler_lat_path = os.path.join(SCALER_SAVE_DIR, 'scaler_lat.joblib')\n",
    "scaler_lon_path = os.path.join(SCALER_SAVE_DIR, 'scaler_lon.joblib')\n",
    "\n",
    "try:\n",
    "    import joblib\n",
    "    # Try to load existing scalers\n",
    "    if os.path.exists(scaler_lat_path) and os.path.exists(scaler_lon_path):\n",
    "        scaler_lat = joblib.load(scaler_lat_path)\n",
    "        scaler_lon = joblib.load(scaler_lon_path)\n",
    "        print(f\"--- Loaded existing scalers from {SCALER_SAVE_DIR} ---\")\n",
    "    else:\n",
    "        print(f\"\\n--- Fitting scalers on TRAINING data: {TRAIN_CSV_PATH} ---\")\n",
    "        # Load only the training data for fitting\n",
    "        train_df_for_scaling = pd.read_csv(TRAIN_CSV_PATH)\n",
    "        if 'latitude' not in train_df_for_scaling.columns or 'longitude' not in train_df_for_scaling.columns:\n",
    "            raise KeyError(\"Training CSV must contain 'latitude' and 'longitude' for scaler fitting.\")\n",
    "\n",
    "        # Fit scalers (use .values.reshape(-1, 1) for single feature)\n",
    "        scaler_lat.fit(train_df_for_scaling[['latitude']].values)\n",
    "        scaler_lon.fit(train_df_for_scaling[['longitude']].values)\n",
    "        print(\"   Scalers fitted successfully on training data.\")\n",
    "\n",
    "        # Save the fitted scalers for inference later\n",
    "        joblib.dump(scaler_lat, scaler_lat_path)\n",
    "        joblib.dump(scaler_lon, scaler_lon_path)\n",
    "        print(f\"   Scalers saved to {SCALER_SAVE_DIR}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Training CSV file not found at {TRAIN_CSV_PATH}. Cannot fit scalers.\")\n",
    "    exit()\n",
    "except KeyError as e:\n",
    "    print(f\"ERROR: {e} Column missing in {TRAIN_CSV_PATH}. Cannot fit scalers.\")\n",
    "    exit()\n",
    "except ImportError:\n",
    "     print(\"WARNING: joblib not found. Cannot save/load scalers. Will fit scalers every run.\")\n",
    "     # Re-fit scalers if joblib is not available (less efficient)\n",
    "     print(f\"\\n--- Fitting scalers on TRAINING data: {TRAIN_CSV_PATH} (joblib not found) ---\")\n",
    "     train_df_for_scaling = pd.read_csv(TRAIN_CSV_PATH)\n",
    "     if 'latitude' not in train_df_for_scaling.columns or 'longitude' not in train_df_for_scaling.columns:\n",
    "            raise KeyError(\"Training CSV must contain 'latitude' and 'longitude' for scaler fitting.\")\n",
    "     scaler_lat.fit(train_df_for_scaling[['latitude']].values)\n",
    "     scaler_lon.fit(train_df_for_scaling[['longitude']].values)\n",
    "     print(\"   Scalers fitted successfully on training data (not saved).\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during scaler handling: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# 3. Transforms\n",
    "# Use normalization values standard for ImageNet pre-trained models\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "# 4. Datasets and DataLoaders\n",
    "print(\"\\n--- Creating Datasets and DataLoaders ---\")\n",
    "try:\n",
    "    train_dataset = GeoImageDataset(csv_file=TRAIN_CSV_PATH,\n",
    "                                    img_dir=TRAIN_IMG_DIR,\n",
    "                                    transform=train_transform,\n",
    "                                    scaler_lat=scaler_lat, # Pass fitted scalers\n",
    "                                    scaler_lon=scaler_lon)\n",
    "\n",
    "    val_dataset = GeoImageDataset(csv_file=VAL_CSV_PATH,\n",
    "                                  img_dir=VAL_IMG_DIR,\n",
    "                                  transform=val_transform,\n",
    "                                  scaler_lat=scaler_lat, # Use the SAME scalers fitted on train data\n",
    "                                  scaler_lon=scaler_lon,\n",
    "                                  exclude_filenames=validation_exclude_filenames) # Pass the list of files to exclude\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=os.cpu_count()//2, pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=os.cpu_count()//2, pin_memory=True)\n",
    "\n",
    "    print(f\"\\nTrain dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "    print(f\"Train DataLoader steps per epoch: {len(train_loader)}\")\n",
    "    print(f\"Validation DataLoader steps per epoch: {len(val_loader)}\")\n",
    "    dataloaders_ready = True\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nERROR Creating Datasets: Required file/directory not found.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    print(f\"Please check paths: TRAIN_IMG_DIR, TRAIN_CSV_PATH, VAL_IMG_DIR, VAL_CSV_PATH\")\n",
    "    dataloaders_ready = False\n",
    "except ValueError as e: # Catch errors from Dataset init (e.g., missing columns)\n",
    "     print(f\"\\nERROR Creating Datasets: {e}\")\n",
    "     dataloaders_ready = False\n",
    "except Exception as e:\n",
    "     print(f\"\\nAn unexpected error occurred creating Dataset/DataLoader: {e}\")\n",
    "     dataloaders_ready = False\n",
    "\n",
    "\n",
    "# 5. Initialize Model\n",
    "if dataloaders_ready: # Only proceed if datasets were loaded correctly\n",
    "    print(\"\\n--- Initializing Combined Geo Model ---\")\n",
    "    try:\n",
    "        model = CombinedGeoModel(\n",
    "            frozen_region_model_path=FROZEN_MODEL_PATH,\n",
    "            num_region_classes=NUM_REGION_CLASSES,\n",
    "            head_hidden_dims=HEAD_HIDDEN_DIMS,\n",
    "            use_softmax_for_region=USE_SOFTMAX_REGION,\n",
    "            trainable_effnet_name=TRAINABLE_EMBEDDER_NAME,\n",
    "            device=DEVICE # Pass the device to the model\n",
    "        )\n",
    "        model_ready = True\n",
    "    except FileNotFoundError as e:\n",
    "         print(f\"\\nERROR Initializing Model: Checkpoint file not found.\")\n",
    "         print(f\"Details: {e}\")\n",
    "         print(f\"Please check path: FROZEN_MODEL_PATH='{FROZEN_MODEL_PATH}'\")\n",
    "         model_ready = False\n",
    "    except (ValueError, RuntimeError) as e: # Catch errors from model init (e.g., unsupported model, load_state_dict issues)\n",
    "         print(f\"\\nERROR Initializing Model: {e}\")\n",
    "         model_ready = False\n",
    "    except Exception as e:\n",
    "         print(f\"\\nAn unexpected error occurred during model initialization: {e}\")\n",
    "         model_ready = False\n",
    "else:\n",
    "    model_ready = False\n",
    "\n",
    "# 6. Loss Function and Optimizer (only if model is ready)\n",
    "if model_ready:\n",
    "    print(\"\\n--- Setting up Loss and Optimizer ---\")\n",
    "    criterion = WeightedLatLonLoss(lat_weight=LAT_LOSS_WEIGHT, lon_weight=LON_LOSS_WEIGHT)\n",
    "    # Only optimize parameters that require gradients\n",
    "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        trainable_params,\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=1e-5 # L2 regularization\n",
    "    )\n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                           mode='min',     # Reduce LR when validation loss stops decreasing\n",
    "                                                           factor=0.1,     # Reduce LR by a factor of 0.1\n",
    "                                                           patience=5,     # Wait 5 epochs of no improvement before reducing\n",
    "                                                           verbose=True)\n",
    "    optimizer_ready = True\n",
    "else:\n",
    "    optimizer_ready = False\n",
    "\n",
    "\n",
    "if dataloaders_ready and model_ready and optimizer_ready:\n",
    "    print(\"\\n--- Starting Training Loop ---\")\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(\"-\" * 30)\n",
    "        print(f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        # --- Training Phase ---\n",
    "        model.train() # Set model to training mode\n",
    "        running_train_loss = 0.0\n",
    "        processed_samples_train = 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += loss.item() * inputs.size(0) # Loss per batch * batch size\n",
    "            processed_samples_train += inputs.size(0)\n",
    "\n",
    "            if (i + 1) % 100 == 0: # Print every 100 mini-batches\n",
    "                avg_batch_loss = running_train_loss / processed_samples_train\n",
    "                print(f'  Train Step [{i+1}/{len(train_loader)}], Avg Loss: {avg_batch_loss:.6f}')\n",
    "\n",
    "        epoch_train_loss = running_train_loss / len(train_dataset)\n",
    "        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}] Average Training Loss: {epoch_train_loss:.6f}')\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        running_val_loss = 0.0\n",
    "        processed_samples_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs_val, labels_val in val_loader:\n",
    "                inputs_val, labels_val = inputs_val.to(DEVICE), labels_val.to(DEVICE)\n",
    "                outputs_val = model(inputs_val)\n",
    "                loss_val = criterion(outputs_val, labels_val)\n",
    "                running_val_loss += loss_val.item() * inputs_val.size(0)\n",
    "                processed_samples_val += inputs_val.size(0)\n",
    "\n",
    "        # Check if validation set was non-empty after exclusion\n",
    "        if len(val_dataset) > 0:\n",
    "            epoch_val_loss = running_val_loss / len(val_dataset)\n",
    "            print(f'Epoch [{epoch+1}/{NUM_EPOCHS}] Validation Loss: {epoch_val_loss:.6f}')\n",
    "\n",
    "            # Step the scheduler based on validation loss\n",
    "            scheduler.step(epoch_val_loss)\n",
    "\n",
    "            # Save the best model based on validation loss\n",
    "            if epoch_val_loss < best_val_loss:\n",
    "                best_val_loss = epoch_val_loss\n",
    "                best_model_path = os.path.join(MODEL_SAVE_DIR, f'best_combined_model.pth')\n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_loss': best_val_loss,\n",
    "                    'scaler_lat_path': scaler_lat_path, # Store path to scalers used\n",
    "                    'scaler_lon_path': scaler_lon_path,\n",
    "                }, best_model_path)\n",
    "                print(f\"   *** New best model saved to {best_model_path} (Val Loss: {best_val_loss:.6f}) ***\")\n",
    "        else:\n",
    "            print(\"   Validation set is empty, skipping validation loss calculation and model saving.\")\n",
    "            # Optionally, save based on training loss if validation is not possible\n",
    "            # scheduler.step(epoch_train_loss) # Or don't step scheduler\n",
    "\n",
    "        # Save checkpoint periodically (e.g., every 5 epochs)\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            checkpoint_path = os.path.join(MODEL_SAVE_DIR, f'combined_model_epoch_{epoch+1}.pth')\n",
    "            torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'train_loss': epoch_train_loss,\n",
    "                    'val_loss': epoch_val_loss if len(val_dataset) > 0 else None,\n",
    "                    'scaler_lat_path': scaler_lat_path, # Store path to scalers used\n",
    "                    'scaler_lon_path': scaler_lon_path,\n",
    "                }, checkpoint_path)\n",
    "            print(f\"   Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "\n",
    "    print('\\nFinished Training')\n",
    "\n",
    "else:\n",
    "    print(\"\\n--- Training Skipped ---\")\n",
    "    if not dataloaders_ready:\n",
    "        print(\"Reason: Dataloaders failed to initialize.\")\n",
    "    elif not model_ready:\n",
    "        print(\"Reason: Model failed to initialize.\")\n",
    "    elif not optimizer_ready:\n",
    "        print(\"Reason: Optimizer setup failed.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
