{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca10052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "\n",
    "\n",
    "class CombinedGeoModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_region_classes=15,\n",
    "                 head_hidden_dims=[512, 128],\n",
    "                 use_softmax_for_region=True,\n",
    "                 effnet_name='efficientnet_b0'):\n",
    "        super().__init__()\n",
    "        self.num_region_classes = num_region_classes\n",
    "        self.use_softmax_for_region = use_softmax_for_region\n",
    "\n",
    "        # 1. Initialize Frozen Region Model STRUCTURE using TIMM\n",
    "        print(f\"--- Initializing Frozen Region Model STRUCTURE ({effnet_name} using TIMM) ---\")\n",
    "        self.frozen_region_model = timm.create_model(\n",
    "            effnet_name, pretrained=False, num_classes=num_region_classes\n",
    "        )\n",
    "        for param in self.frozen_region_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.frozen_region_model.eval()\n",
    "        print(\"Frozen Region Model (TIMM) structure initialized and frozen.\")\n",
    "\n",
    "        # 2. Initialize Trainable Image Embedding Model STRUCTURE using TIMM\n",
    "        print(f\"\\n--- Initializing Trainable Image Embedder STRUCTURE ({effnet_name} using TIMM) ---\")\n",
    "        self.trainable_image_embedder = timm.create_model(\n",
    "            effnet_name, pretrained=False # Weights loaded from state_dict\n",
    "        )\n",
    "        self.embedding_dim = self.trainable_image_embedder.get_classifier().in_features\n",
    "        self.trainable_image_embedder.reset_classifier(0, '') # Remove classifier\n",
    "        print(f\"Trainable Image Embedder (TIMM) structure initialized (output dim after pool: {self.embedding_dim}).\")\n",
    "\n",
    "        # ADD Global Pooling Layer\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        print(\"Added AdaptiveAvgPool2d layer for trainable embedder features.\")\n",
    "\n",
    "        # 3. Define the Regression Head\n",
    "        print(\"\\n--- Defining Regression Head ---\")\n",
    "        input_head_dim = self.embedding_dim + self.num_region_classes\n",
    "        layers = []\n",
    "        current_dim = input_head_dim\n",
    "        for hidden_dim in head_hidden_dims:\n",
    "           layers.extend([\n",
    "                nn.Linear(current_dim, hidden_dim), nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(), nn.Dropout(0.3)\n",
    "            ])\n",
    "           current_dim = hidden_dim\n",
    "        layers.append(nn.Linear(current_dim, 2))\n",
    "        self.regression_head = nn.Sequential(*layers)\n",
    "        print(f\"Regression head structure defined (Input: {input_head_dim}, Output: 2).\")\n",
    "\n",
    "        print(\"\\nCombinedGeoModel Structure Initialization Complete.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Get Region Features (2D)\n",
    "        with torch.no_grad():\n",
    "            region_logits = self.frozen_region_model(x)\n",
    "            region_features = F.softmax(region_logits, dim=1) if self.use_softmax_for_region else region_logits\n",
    "\n",
    "        # 2. Get Image Embeddings (Apply Pooling)\n",
    "        image_feature_map = self.trainable_image_embedder(x) # Output is 4D\n",
    "        pooled_features = self.global_pool(image_feature_map) # Pool to (B, C, 1, 1)\n",
    "        image_features = torch.flatten(pooled_features, 1) # Flatten to (B, C)\n",
    "\n",
    "        # 3. Concatenate features (Both 2D now)\n",
    "        combined_features = torch.cat((image_features, region_features), dim=1)\n",
    "\n",
    "        # 4. Pass through the regression head\n",
    "        output = self.regression_head(combined_features)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ffd8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import joblib\n",
    "#from your_model_definition_file import CombinedGeoModel  # Adjust import if needed\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "INPUT_CSV_PATH = \"/kaggle/input/latlong-dataset/train_combine.csv\"  # Only 'filename' column\n",
    "IMG_DIR_PATH = \"/kaggle/input/latlong-dataset/images_train_combine\"\n",
    "OUTPUT_CSV_PATH = \"predicted_latlon_train.csv\"\n",
    "TRAINED_MODEL_PATH = \"/kaggle/working/combined_model_checkpoints/best_combined_model_finetuned.pth\"\n",
    "SCALER_LAT_PATH = \"/kaggle/working/scaler_lat.joblib\"\n",
    "SCALER_LON_PATH = \"/kaggle/working/scaler_lon.joblib\"\n",
    "NUM_REGION_CLASSES = 15\n",
    "EFFNET_NAME = 'efficientnet_b0'\n",
    "HEAD_HIDDEN_DIMS = [512, 256, 128]\n",
    "USE_SOFTMAX_FROM_FROZEN = True\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 2\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Dataset (No Lat/Lon) ===\n",
    "class FilenameOnlyDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_filename = self.data_frame.iloc[idx]['filename']\n",
    "        img_path = os.path.join(self.img_dir, img_filename)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform: image_tensor = self.transform(image)\n",
    "        else: image_tensor = transforms.ToTensor()(image)\n",
    "        return image_tensor, img_filename\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, filenames = zip(*batch)\n",
    "    return torch.stack(images), filenames\n",
    "\n",
    "# === Transformations ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# === Load Scalers ===\n",
    "scaler_lat = joblib.load(SCALER_LAT_PATH)\n",
    "scaler_lon = joblib.load(SCALER_LON_PATH)\n",
    "\n",
    "# === Load Model ===\n",
    "model = CombinedGeoModel(\n",
    "    num_region_classes=NUM_REGION_CLASSES,\n",
    "    head_hidden_dims=HEAD_HIDDEN_DIMS,\n",
    "    use_softmax_for_region=USE_SOFTMAX_FROM_FROZEN,\n",
    "    effnet_name=EFFNET_NAME\n",
    ")\n",
    "checkpoint = torch.load(TRAINED_MODEL_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "model.to(DEVICE); model.eval()\n",
    "\n",
    "# === Inference ===\n",
    "dataset = FilenameOnlyDataset(INPUT_CSV_PATH, IMG_DIR_PATH, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=collate_fn)\n",
    "\n",
    "results = []\n",
    "with torch.no_grad():\n",
    "    for images, filenames in dataloader:\n",
    "        images = images.to(DEVICE)\n",
    "        preds = model(images)  # (N, 2)\n",
    "        pred_np = preds.cpu().numpy()\n",
    "        lat_pred = scaler_lat.inverse_transform(pred_np[:, 0].reshape(-1, 1)).flatten()\n",
    "        lon_pred = scaler_lon.inverse_transform(pred_np[:, 1].reshape(-1, 1)).flatten()\n",
    "        results.extend(zip(filenames, lat_pred, lon_pred))\n",
    "\n",
    "# === Save to CSV ===\n",
    "df_out = pd.DataFrame(results, columns=['filename', 'predicted_latitude', 'predicted_longitude'])\n",
    "df_out.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "print(f\"âœ… Predictions saved to {OUTPUT_CSV_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
